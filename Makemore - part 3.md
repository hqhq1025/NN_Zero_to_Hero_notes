#æ·±åº¦å­¦ä¹  #Zero2hero

![](https://www.youtube.com/watch?v=P6sfmUTpUmc&t=241s)

# åˆå§‹åŒ–
[[../åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ /4.8 æ•°å€¼ç¨³å®šæ€§å’Œæ¨¡å‹åˆå§‹åŒ–]]
## ä¸åˆç†çš„éšæœºåˆå§‹åŒ–
### å½“æ¦‚ç‡ç›¸ç­‰
å›é¡¾ä¸Šä¸€æ¬¡çš„æ¨¡å‹, æˆ‘ä»¬å‘ç°ç¬¬ä¸€ä¸ª epoch çš„ loss å·¨å¤§ (27), ä½†æ˜¯åœ¨ç¬¬äºŒä¸ª epoch ä¸­ä¾¿çªç„¶ä¸‹é™åˆ° 2, è¿™è¯´æ˜æˆ‘ä»¬çš„åˆå§‹åŒ–å¹¶ä¸åˆç†:
AK åšäº†ä¸€ä¸ªå°è¯•, æŠŠæ¦‚ç‡è®¾ä¸º 1/27 ä¹Ÿå°±æ˜¯æ‰€æœ‰å­—ç¬¦ç­‰æ¦‚ç‡å‡ºç°, æŒ‰ç…§æ­¤æ¦‚ç‡è®¡ç®— loss, ä¹Ÿæ‰ 3 å‡ºå¤´
### è°ƒæ•´ W å’Œ b
è¿™é‡Œæˆ‘ä»¬å¯ä»¥ä¸ºä¸€å¼€å§‹åˆå§‹åŒ–çš„ W, b åšå‡ºè°ƒæ•´, æ¯”å¦‚ä¹˜ä»¥ 0, è¿™æ ·ä¸€æ¥å½“æˆ‘ä»¬ç”¨ softmax è®¡ç®—, å¾—åˆ°çš„å°±æ˜¯: 1/27, 1/27, ..., 1/27 è¿™æ ·çš„ list, ä¹Ÿå°±æ˜¯ç­‰æ¦‚ç‡
å†ç»è¿‡äº¤å‰ç†µè®¡ç®— loss, ä¾¿å¾—åˆ°äº† 3 å‡ºå¤´çš„ loss, ä¹Ÿå°±æ˜¯ç­‰æ¦‚ç‡å¯¹åº”çš„ loss
### W å¿…é¡»éé›¶
æœ€å AK æŠŠ W çš„ç¼©æ”¾å› å­ä» 0 æ”¹æˆäº† 0.01ï¼Œè™½ç„¶åˆå§‹ loss ç•¥å¤§äºç­‰æ¦‚ç‡è¾“å‡ºï¼Œä½†è¿™æ˜¯å› ä¸º softmax è¾“å‡ºä¸å†å®Œå…¨å‡åŒ€ï¼Œ**äº¤å‰ç†µå¯¹åå·®æ›´åŠ æ•æ„Ÿ**ã€‚ä½†è¿™åè€Œä¸ºæ¨¡å‹æä¾›äº†â€œå­¦ä¹ ç©ºé—´â€ã€‚
æˆ‘çš„ç†è§£æ˜¯ï¼š**å½“ W ä¸ºé›¶æ—¶ï¼Œæ¨¡å‹è¾“å‡ºç¨³å®šï¼Œä½†å®Œå…¨æ— æ³•å­¦ä¹ **ã€‚ç›¸åï¼Œb è®¾ç½®ä¸º 0 æ˜¯ä¸€ç§å¸¸è§çš„åˆå§‹åŒ–æ–¹æ³•ï¼Œå¯¹è®­ç»ƒæ²¡æœ‰è´Ÿé¢å½±å“ã€‚
#### ä¸ºä»€ä¹ˆæ— æ³•å­¦ä¹ ï¼Ÿ
è¿™éœ€è¦å›æº¯åˆ°åå‘ä¼ æ’­çš„åŸç†ï¼š
å½“ `W` è¢«ä¹˜ä»¥ `0` åï¼Œå…¶å®æ˜¯è¢« PyTorch è§†ä¸ºä¸€ä¸ªæ–°çš„å¸¸é‡å¼ é‡ï¼Œ**å¹¶æœªæŒ‚åœ¨åŸæœ‰çš„è®¡ç®—å›¾**ä¸Šã€‚è¿™å¯¼è‡´åœ¨åå‘ä¼ æ’­æ—¶ `W` æ— æ³•æ¥æ”¶åˆ°æ¥è‡ª loss çš„æ¢¯åº¦æ›´æ–°ã€‚å³ä½¿åç»­äººä¸ºè®¾ç½®äº† `requires_grad=True`ï¼Œä¹Ÿæ— æ³•è¡¥æ•‘è¿™ä¸€ç‚¹ â€”â€” å› ä¸º**æ¢¯åº¦è·¯å¾„å·²ç»æ–­å¼€**äº†ã€‚
# æ¿€æ´»å‡½æ•°
## æ¿€æ´»ä¸æ¢¯åº¦
æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¥è®¨è®ºæ¿€æ´»å‡½æ•° `tanh` çš„è¡Œä¸ºåŠå…¶å¯¹æ¢¯åº¦ä¼ æ’­çš„å½±å“ã€‚

è¿›å…¥æºä»£ç å¯ä»¥çœ‹åˆ° `tanh` çš„è®¡ç®—å…¬å¼ï¼š
$$
\tanh(x) = \frac{e^{2x} - 1}{e^{2x} + 1}
$$
è¯¥å‡½æ•°çš„å¯¼æ•°ä¸ºï¼š
$$
\frac{d}{dx} \tanh(x) = 1 - \tanh^2(x)
$$
ä»è¿™ä¸ªå…¬å¼å¯ä»¥çœ‹å‡ºï¼š
- å½“ $\tanh(x) \approx \pm1$ï¼ˆè¾“å‡ºæ¥è¿‘é¥±å’ŒåŒºï¼‰æ—¶ï¼Œå¯¼æ•°è¶‹è¿‘äº 0ï¼Œåå‘ä¼ æ’­æ—¶æ¢¯åº¦å‡ ä¹æ— æ³•ä¼ é€’ï¼›
  - è¿™è¡¨ç¤ºç¥ç»å…ƒå·²ç»â€œéå¸¸ç¡®å®šâ€ï¼Œå‡ ä¹ä¸å†å­¦ä¹ ï¼ˆæ¢¯åº¦ * 0ï¼‰ï¼›
- å½“ $\tanh(x) \approx 0$ï¼ˆè¾“å…¥åœ¨çº¿æ€§åŒºï¼‰æ—¶ï¼Œå¯¼æ•°æ¥è¿‘ 1ï¼Œæ¢¯åº¦å¯ä»¥å®Œæ•´ä¼ é€’ï¼›
  - æ­¤æ—¶ç¥ç»å…ƒâ€œæœªæ¿€æ´»â€ï¼Œä½†æ›´å®¹æ˜“è¢«è®­ç»ƒï¼ˆæ¢¯åº¦ * 1ï¼‰
å› æ­¤ï¼Œ**â€œæ¿€æ´»â€æŒ‡çš„æ˜¯æ­£å‘è¾“å‡ºçš„çŠ¶æ€ï¼Œè€Œä¸æ˜¯æ˜¯å¦èƒ½è®­ç»ƒï¼›  
æ¢¯åº¦æ˜¯å¦èƒ½é€šè¿‡ï¼Œå–å†³äºå½“å‰æ¿€æ´»å‡½æ•°çš„å¯¼æ•°å€¼ã€‚**

---

## è°ƒæ•´ W å’Œ b

é€šè¿‡å¯è§†åŒ–æ¯ä¸ªç¥ç»å…ƒçš„æ¿€æ´»å€¼åˆ†å¸ƒï¼Œå¯ä»¥å‘ç°å¦‚æœä½¿ç”¨é»˜è®¤åˆå§‹åŒ–çš„ `Wâ‚` å’Œ `bâ‚`ï¼Œå¾ˆå¤šç¥ç»å…ƒçš„è¾“å‡ºç›´æ¥è½åœ¨ tanh çš„é¥±å’ŒåŒºï¼ˆå³æ¥è¿‘ Â±1ï¼‰ã€‚

è¿™ä¼šå¯¼è‡´ï¼š
- æ­£å‘è¾“å‡ºçœ‹ä¼¼â€œæ¿€æ´»â€ï¼Œä½†
- åå‘ä¼ æ’­ä¸­å¯¼æ•°è¶‹è¿‘ 0ï¼Œ**æ¢¯åº¦å‡ ä¹æ— æ³•æ›´æ–°å‚æ•°**ã€‚

å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ï¼š
- ç¼©å°æƒé‡ W çš„åˆå§‹å€¼ï¼ˆå¦‚ä¹˜ä»¥ 0.1ï¼‰ï¼›
- ç¼©å°åç½® b çš„åˆå§‹å€¼ï¼ˆå¦‚ä¹˜ä»¥ 0.01ï¼‰ï¼›
è¿™æ ·å¯ä»¥ä½¿è¾“å…¥é›†ä¸­åœ¨ tanh çš„**çº¿æ€§å“åº”åŒº**ï¼ˆè¾“å‡ºæ¥è¿‘ 0ï¼‰ï¼Œ  
ä»è€Œæå‡ç¥ç»å…ƒçš„å¯è®­ç»ƒæ€§ï¼Œç¡®ä¿æ¢¯åº¦èƒ½é¡ºåˆ©ä¼ é€’ã€‚

---

### ğŸ§  å°ç»“ï¼š
- Tanh è¾“å‡ºæ¥è¿‘ Â±1 â†’ æ¿€æ´»å¼ºï¼Œä½†æ¢¯åº¦å°ï¼›
- Tanh è¾“å‡ºæ¥è¿‘ 0 â†’ æœªæ¿€æ´»ï¼Œä½†æ¢¯åº¦å¤§ â†’ æ›´å®¹æ˜“è®­ç»ƒï¼›
- åˆç†åˆå§‹åŒ– W å’Œ bï¼Œæœ‰åŠ©äºè®©ç¥ç»å…ƒè¿›å…¥â€œå®¹æ˜“è®­ç»ƒâ€çš„åŒºåŸŸã€‚
# Kaiming åˆå§‹åŒ–
## ä¿æŒåˆ†å¸ƒæƒ…å†µ
ç¥ç»ç½‘ç»œä¸­çš„æ•°å€¼åœ¨æ¯å±‚ä¹‹é—´ä¼ é€’æ—¶, å…¶**åˆ†å¸ƒæƒ…å†µä¼šå‘ç”Ÿæ”¹å˜** (æ¯”å¦‚è¢«åˆ†å¸ƒç›´æ–¹å›¾å‹ç¼©/è¢«æ‹‰ä¼¸), å› æ­¤æˆ‘ä»¬éœ€è¦é€šè¿‡è§‚å¯Ÿæ•°æ®åˆ†å¸ƒ, å¹¶è°ƒæ•´,  **ä½¿å¾—æ¯ä¸€å±‚è¾“å‡ºçš„æ–¹å·®ä¿æŒç¨³å®š**, é¿å…å‰å‘/åå‘ä¼ æ’­ä¸­ä¿¡æ¯è¢«æ”¾å¤§/æ¶ˆå‡
æˆ‘ä»¬å¯ä»¥é€šè¿‡ç»™æƒé‡çŸ©é˜µä¹˜ä»¥æŸä¸ªå€¼, å¯¹åˆ†å¸ƒè¿›è¡Œç¼©æ”¾, é‚£ä¹ˆé—®é¢˜æ¥äº†: å¦‚ä½•ç¡®å®šå…·ä½“çš„å€¼å‘¢?
## å…³äº ReLU
![[../../source/ReLU function.png]]
Relu å‡½æ•°æ˜¯ä¸€ä¸ª squashing function (æŒ¤å‹å‡½æ•°)
è¿™æ˜¯ ReLU å‡½æ•°çš„æ›²çº¿: æ‰€æœ‰å¤§äº 0 çš„æ•°æ­£å¸¸è¾“å‡º, æ‰€æœ‰å°äº 0 çš„æ•°è¢«æˆªæ–­ä¸º 0
ç”±äºä¸¢æ‰äº†ä¸€åŠçš„åˆ†å¸ƒ, Kaiming å‘ç°éœ€è¦ç”¨ä¸€äº› gain (å¢ç›Š) è¿›è¡Œè¡¥å¿
## Gain å¢ç›Š
Kaiming å‘ç°, éœ€è¦åœ¨ç”¨é«˜æ–¯åˆ†å¸ƒåˆå§‹åŒ–æƒé‡çŸ©é˜µæ—¶, éœ€è¦ä¹˜ä»¥ä¸€ä¸ª gain, å…·ä½“å¤§å°æ˜¯: $$\sqrt{2/n_l}$$
å…¶ä¸­: 2 ä¸»è¦æ˜¯ä¸ºäº†å¼¥è¡¥ ReLU å‡½æ•°ä¸¢æ‰çš„ä¸€åŠå¯¹äºåˆ†å¸ƒé€ æˆçš„å½±å“, $n_{l}$ æŒ‡çš„æ˜¯ **fan_in**, ä¹Ÿå°±æ˜¯**è¾“å‡ºç¥ç»å…ƒè¿æ¥çš„è¾“å…¥æ•°**
## PyTorch ä¸­çš„ Kaiming åˆå§‹åŒ–
```python 
torch.nn.init.kaiming_normal_( tensor, a=0, mode='fan_in', nonlinearity='leaky_relu' )
```
å¯¹äºä¸åŒçš„éçº¿æ€§æ¿€æ´»å‡½æ•°, gain æœ‰ç€ä¸åŒçš„è®¡ç®—æ–¹å¼
åœ¨ Kaiming åˆå§‹åŒ–æå‡ºä¹‹å‰, é€‰æ‹©æ¿€æ´»å‡½æ•°, è®¾ç½®ç›¸å…³çš„ gain å¿…é¡»ååˆ†è°¨æ…, åå¤å¾®è°ƒ. åœ¨ç»™å‡ºäº†æ¯ä¸€ç§æ¿€æ´»å‡½æ•°å¯¹åº”çš„ gain ä¹‹å, å¤§å¤§çš„ä¾¿åˆ©äº†åˆå§‹åŒ–çš„è®¾ç½®.

## å…³äº fan
â€œ**fan**â€å…¶å®æ˜¯ä»â€œ**fan-out / fan-in of a neuron**â€è¿™ç±»æœ¯è¯­ä¸­æ¥çš„ï¼Œæºè‡ªç”µå­å·¥ç¨‹å’Œç¥ç»ç½‘ç»œçš„æ—©æœŸç ”ç©¶ã€‚
> **fan**Â åœ¨è¿™é‡Œçš„å«ä¹‰æ˜¯â€œ**æ‰‡å‡º**â€æˆ–â€œ**æ‰‡å…¥**â€çš„æ„æ€ï¼Œå°±åƒé£æ‰‡çš„å¶ç‰‡ï¼ˆfan bladeï¼‰é‚£æ ·å‘å¤–/å‘å†…å±•å¼€è¿æ¥ã€‚
### ç¥ç»ç½‘ç»œä¸­çš„ç”¨æ³•
- æ¯ä¸ªç¥ç»å…ƒå¯èƒ½è¿æ¥å¾ˆå¤šå‰ä¸€å±‚çš„è¾“å‡ºï¼Œè¿™äº›è¾“å…¥çš„æ•°é‡å«Â **fan-in**ï¼›
- å®ƒå¯èƒ½è¿æ¥å¾ˆå¤šåä¸€å±‚çš„ç¥ç»å…ƒï¼Œå«Â **fan-out**ã€‚
### è®¡ç®—æ–¹æ³•
ä¸¾ä¸ªä¾‹å­ï¼š
```python
nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3)
```
- fan_in = 3 Ã— 3 Ã— 3 = 27
- fan_out = 64 Ã— 3 Ã— 3 = 576

# Batch Normalization (BN)
 é€šè¿‡è°ƒæ•´æƒé‡çŸ©é˜µçš„åˆ†å¸ƒ, æå‡äº†æ¨¡å‹æ•ˆæœå, æˆ‘ä»¬è¿›ä¸€æ­¥æƒ³: èƒ½å¦é€šè¿‡è°ƒæ•´æ¯ä¸ª mini-batch ä¸­æ•°æ®çš„åˆ†å¸ƒ, è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹? è¿™ç§æ–¹æ³•ä¾¿ç§°ä¸º BN, **Batch Normalization**.
## ç›®çš„ 
- ç¼“è§£ **Internal Covariate Shift**ï¼ˆå†…éƒ¨åå˜é‡åç§»ï¼‰ 
- åŠ é€Ÿè®­ç»ƒï¼Œæå‡ç¨³å®šæ€§ 
- é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤± 
- å…è®¸æ›´å¤§çš„å­¦ä¹ ç‡ã€æ›´å¿«æ”¶æ•›
## è®¡ç®—å…¬å¼
ï¼ˆä»¥ä¸€ä¸ªç¥ç»å…ƒç»´åº¦ä¸ºä¾‹ï¼‰ è®¾ä¸€ä¸ª mini-batch ä¸­æœ‰ \( m \) ä¸ªæ ·æœ¬ï¼ŒæŸä¸€å±‚çš„æŸä¸ªç¥ç»å…ƒè¾“å‡ºä¸ºï¼š
$$ x_1, x_2, \dots, x_m $$
### ç¬¬ä¸€æ­¥ï¼šè®¡ç®— mini-batch çš„å‡å€¼ 
$$ \mu_B = \frac{1}{m} \sum_{i=1}^{m} x_i  $$
### ç¬¬äºŒæ­¥ï¼šè®¡ç®— mini-batch çš„æ–¹å·® 
$$\sigma_B^2 = \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu_B)^2 $$
### ç¬¬ä¸‰æ­¥ï¼šæ ‡å‡†åŒ– 
$$ \hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} $$
 å…¶ä¸­ $\epsilon$ æ˜¯ä¸€ä¸ªå¾ˆå°çš„æ­£æ•°ï¼Œç”¨äºé˜²æ­¢é™¤ä»¥ 0ã€‚ 
### ç¬¬å››æ­¥ ï¼šç¼©æ”¾å’Œå¹³ç§»ï¼ˆå¯å­¦ä¹ å‚æ•°ï¼‰
  $$ y_i = \gamma \hat{x}_i + \beta$$
å…¶ä¸­ $\gamma$ å’Œ $\beta$ æ˜¯å¯å­¦ä¹ å‚æ•°ï¼Œæ¢å¤ç½‘ç»œçš„è¡¨è¾¾èƒ½åŠ›ã€‚
è¿™å…¶å®ä¹Ÿæ˜¯ç®—ä½œä¸€ç§å¯¹äºæ”¹å˜ batch ä¸­æ•°æ®åŸå§‹åˆ†å¸ƒè€Œåšå‡ºçš„ä¸€äº›"è¡¥å¿", è®©æ¨¡å‹å¯ä»¥è‡ªå·±è°ƒæ•´è¡¨è¾¾èƒ½åŠ›
  
æ³¨æ„: **BN ç”¨äºæ•°æ®è¿›å…¥æ¿€æ´»å‡½æ•°ä¹‹å‰**, ä½¿å¾—ä¸è¦è®©ç¥ç»å…ƒè¢«æ¿€æ´»å¾—å¤ªâ€œè¿‡ç«â€æˆ–â€œå¤±è”â€ï¼Œè€Œæ˜¯å°½é‡ä¿æŒè¾“å…¥åˆ†å¸ƒç¨³å®šï¼Œå‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸º 1ï¼Œ**è®©æ¿€æ´»å‡½æ•°å¤„äºâ€œæœ‰æ•ˆå·¥ä½œåŒºé—´â€**
## æ¨ç†æ—¶çš„ BN
åˆšæ‰çš„ BN å…¶å®æ˜¯é’ˆå¯¹è®­ç»ƒæ—¶çš„æ¯ä¸€ä¸ª Batch, ç„¶è€Œåœ¨æ¨ç†æ—¶, è¾“å…¥çš„æ•°æ®å¾ˆå°‘, å¦‚æœä»…ä»…æŠŠè¾“å…¥æ•°æ®ä½œä¸º mini-batch æ¥è¿›è¡Œ BN çš„è¯, ä¼šæœ‰å¾ˆå¤§çš„è¯¯å·®, æ‰€ä»¥æˆ‘ä»¬éœ€è¦å°è¯•å…¶ä»–çš„æ–¹æ³•
### å…¨å±€ BN
æˆ‘ä»¬å¯ä»¥é‡‡ç”¨åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šè®¡ç®— bn ç›¸å…³å€¼, ç”±äºåç»­æ¨ç†æ—¶è¿›è¡Œ BN:
```python
# calibrate the batch norm at the end of training
with torch.no_grad():
  # pass the training set through
  emb = C[Xtr]
  embcat = emb.view(emb.shape[0], -1)
  hpreact = embcat @ W1 # + b1
  # measure the mean/std over the entire training set
  bnmean = hpreact.mean(0, keepdim=True)
  bnstd = hpreact.std(0, keepdim=True)

```
è¿™é‡Œä¸è®°å½•æ¢¯åº¦, å› ä¸ºä¸éœ€è¦æ„å»ºè®¡ç®—å›¾ç”¨äºåå‘ä¼ æ’­ä¸­æ›´æ–°å‚æ•°, åªæ˜¯å•ç‹¬çš„è®¡ç®—è€Œå·²
è¿™é‡Œæ˜¯å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œ mean å’Œ std çš„æ“ä½œ
### å¦ä¸€ç§è¿‘ä¼¼å…¨å±€ BN çš„æ–¹æ³•
æˆ‘ä»¬å¯ä»¥æ ¹æ®æ¯ä¸ª minibatch çš„æ•°æ®, è¿­ä»£æ›´æ–° mean å’Œ std, å¾—åˆ°å’Œå…¨å±€ BN æå…¶è¿‘ä¼¼çš„å€¼, å¯ä»¥å¹³æ›¿, èƒ½å¤ŸèŠ‚çœè®¡ç®—èµ„æº
#### åˆå§‹åŒ–
å¯ä»¥åœ¨åˆå§‹åŒ–çš„æ—¶å€™è®¾ç½®å¥½ bn ç›¸å…³çš„å˜é‡, ç”¨äºåç»­è®¡ç®—å’Œæ›´æ–°
```python
# BatchNorm parameters
bngain = torch.ones((1, n_hidden))
bnbias = torch.zeros((1, n_hidden))
bnmean_running = torch.zeros((1, n_hidden))
bnstd_running = torch.ones((1, n_hidden))
```
è¿™é‡Œçš„ bngain å’Œ bnbias æ˜¯ä½œä¸ºå˜é‡, å‚ä¸åç»­åå‘ä¼ æ’­çš„æ›´æ–°çš„
mean å’Œ std åœ¨è¿™é‡Œæ˜¯running, æ˜¯éšç€æ¯ä¸ª batch çš„éå†è¿›è¡Œæ›´æ–°, åœ¨åˆå§‹åŒ–æ—¶åˆ†åˆ«ä½¿ç”¨ `zeros` å’Œ `ones` çš„æ–¹æ³•
#### mini-batch ä¸­çš„æ›´æ–°
```python
bnmeani = hpreact.mean(0, keepdim=True)
bnstdi = hpreact.std(0, keepdim=True)
hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias
with torch.no_grad():
	bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani
	bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi
```
è¿™æ˜¯åœ¨éå†æ¯ä¸ª minibatch æ—¶çš„ä»£ç , è¿™é‡Œçš„ meani å’Œ stdi æ˜¯æŒ‡çš„å½“å‰ batch çš„ bn ç›¸å…³çš„å€¼, ç„¶åæ ¹æ®å…¬å¼è®¡ç®—å‡º hpreact
åŒæ—¶, æ‰“å¼€ nograd, å› ä¸ºè¿™é‡Œçš„è®¡ç®—ä¸éœ€è¦è®°å½•è®¡ç®—å›¾ç”¨äºåå‘ä¼ æ’­
åœ¨æ ‡å‡†çš„Â **Batch Normalization è®­ç»ƒæµç¨‹**ä¸­ï¼Œæ¯æ¬¡ç”¨å½“å‰ mini-batch çš„å‡å€¼å’Œæ–¹å·®åšå½’ä¸€åŒ–ï¼Œä½†åŒæ—¶æˆ‘ä»¬ä¹Ÿç»´æŠ¤ä¸€ä¸ªâ€œæ»‘åŠ¨å¹³å‡â€çš„å…¨å±€ç»Ÿè®¡é‡
è¿™å°±æ˜¯æ‰€è°“çš„Â **exponential moving averageï¼ˆEMAï¼‰**ï¼Œå…¶ç›®çš„æ˜¯ï¼š
> ç´¯ç§¯å¤šä¸ª batch çš„ç»Ÿè®¡é‡ï¼Œä½œä¸ºæ¨ç†æ—¶çš„ç¨³å®šä¼°è®¡ã€‚
## å¤šä½™çš„ bias
é€šè¿‡è§‚å¯Ÿä»£ç å’Œå…¬å¼, æˆ‘ä»¬å¯ä»¥å‘ç°
```python
# Linear layer
hpreact = embcat @ W1 #+ b1 # hidden layer pre-activation
# BatchNorm layer
# -------------------------------------------------------------
bnmeani = hpreact.mean(0, keepdim=True)
bnstdi = hpreact.std(0, keepdim=True)
hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias
```
åŸæœ‰çš„è¿™ä¸ª b1, åœ¨ç»è¿‡ mean, å¹¶åšå·®æƒ³å‡å, æ­£å¥½æŠµæ¶ˆäº†
å› æ­¤å®ƒå¹¶ä¸ä¼šåœ¨è®¡ç®—å›¾ä¸­å‡ºç°, è¿›è€Œä¸ä¼šåœ¨åå‘ä¼ æ’­ä¸­æ›´æ–°å€¼, æ‰€ä»¥è¿™ä¸ª b1 æ ¹æœ¬æ²¡æœ‰å¿…è¦
è¿›ä¸€æ­¥, **æ‰€æœ‰ BN å±‚éƒ½ä¸éœ€è¦åŸæœ¬çš„ bias**
### é‚£è°æ¥å‘æŒ¥ bias çš„ä½œç”¨ ?
è¿™æ˜¯æˆ‘ä¸€å¼€å§‹çš„ç–‘æƒ‘, ä¸ gpt äº¤æµåå¾—åˆ°äº†è§£ç­”:
å› ä¸º BN å±‚ä¸­å…¶å®æœ‰ `bnbias`, ä¸”æ˜¯å‚æ•°, å¯ä»¥åœ¨åç»­è¢«æ›´æ–°, ç›¸å½“äºå‘æŒ¥äº†åŸæœ¬çº¿æ€§å±‚ä¸­ bias çš„ä½œç”¨